---
title: "Assesssment 1 - COVID-19 impact on digital learning"
author: "Nguyen Khuat Son Tra and 48144134"
date: "2024-04-11"
output: word_document
---


Over the three years since the onset of COVID-19 lockdowns at the beginning of 2020, significant changes have unfolded. One notable transformation is the prominent role of online learning in the educational landscape. While face-to-face classes continues to coexist, online platforms have become integral in enhancing the overall learning experience.

Upon reflecting on three major themes emerging one year after the onset of COVID-19 – namely, the exacerbation of pre-existing inequalities and shortcomings in the educational system, the advocacy for transformative learning, and the demand for innovative approaches in community and intergenerational learning (Stanistreet et al., 2020) – the dataset examined in this report will offer insights into the impact of COVID-19 on learning platforms across various regions in the United States.


# 1. Data Cleaning and Wrangling  

To begin, we set up the working directory and import neccessary library to prepare for the work.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse) # import necessary library
library(knitr)
library(tinytex)

setwd("~/BUSA8000/Assignment 1") # setting up the working directory
```

### 1.1 Data Cleaning
Respectively, we will clean 5 engagement files (1000.csv, 1039.csv, 1044.csv, 1052.csv, and 1131.csv), products_info.csv and districts_info.csv). 

##### Context
After reviewing all engagement datasets, we have decided to exclude the dataset "1131.csv" to reduce noise in the dataset. While both "1131.csv" and "1039.csv" lack district information, we observed that while "1039.csv" is marked as "NaN", the state for "1131.csv" is "don\x92t know". Given the guidelines, "NaN" indicates that these values were intentionally suppressed to enhance dataset anonymization. By retaining one representative of the "NaN" state in our analysis, we maintain dataset integrity without introducing excessive noise. 

##### Engagement Data
Between 1.01.2020 (few months before the pandemic outbreak), and 31.12.2020, five engagement files were observed. Each file within the engagement data captured various product IDs used during the mentioned period within a single district.

**a. 1000.csv (Connecticut)**.
To begin, we observe the dataset: 1000.csv by examining a few initial rows and its columns.

```{r, summary-1000, echo=TRUE}
engage1 = read_csv(file="Engagement Data/1000.csv") #rename for clarity
colSums(is.na(engage1)) # check n/a in each column
```

In the next step, we start to look into each column (time, lp_id, pct_access, and engagement_index). 


Every time we clean, the 'unique()' function was used initially to observe the data and later to verify if changes were made. However, due to word limitations, we do not include this information in the following. 

```{r, column-1000, echo=TRUE}
  ## transform the data: time and lp_id column
engage1 <- engage1 %>%
  mutate(time = replace(time, time == "1/01/2022", "1/01/2020")) %>%
  mutate(time = replace(time, time == "31/12/1020", "31/12/2020")) %>%
  filter(!is.na(lp_id)) # dropping n/a

colSums(is.na(engage1)) # verify if n/a has been filtered

```

Given the unique nature of lp_id, individually checking all 104,003 observations is not efficient. Hence, we only clean and drop n/a values for the first two columns to avoid inadvertently removing necessary time and lp_id data, vital for later merging and visualization. Notably, the 'engagement_index' column contains 42,346 n/a values, almost half the dataset. Thus, removing them all would compromise the integrity of 'time' and 'lp_id' information.


Finally, for the wrangling process, we add an column named 'district_id'.
```{r, district-id-1000, echo=TRUE}
# add district_id column to prepare for the merging step
engage1 <- mutate(engage1, district_id = 1000)
```

**b. 1039.csv (NaN)**.
Similarly, we repeat the process for 1039.csv.
```{r, summary-1039, echo=TRUE}
engage2 = read_csv(file="Engagement Data/1039.csv") #rename for clarity
colSums(is.na(engage2)) # check n/a
```

As we learned from the first dataset, cleaning all n/a values in the pct_access and engagement_index columns can affect our analysis later. Therefore, from now on, we will only perform cleaning for the time and lp_id columns.

```{r, time-1039, echo=TRUE}
engage2 <- engage2 %>%
  mutate(time = replace(time, time == "1/1/2044", "1/1/2020"))
```

```{r, lp-id-1039, echo=TRUE}
## While checking, we found a 'string' value, which poses a risk of disrupting the data. 
## Therefore, it is important to convert it to n/a before dropping it.
engage2 <- engage2 %>%
  mutate(time = replace(time, time == "not sure", NA)) %>%
  mutate(time = replace(time, time == " ", NA)) %>%
  filter(!is.na(lp_id)) %>%
  mutate(`lp_id` = as.numeric(`lp_id`)) # convert for merging

colSums(is.na(engage2)) # verify if adjustment is made

```

```{r, district-id-1039, echo=TRUE}
# add new district_id column
engage2 <- mutate(engage2, district_id = 1039)
```

**c. 1044.csv (Missouri)**.
Similarly, we repeat the process for "1044.csv".
```{r, summary-1044, echo=TRUE}
engage3 = read_csv(file="Engagement Data/1044.csv") #rename for clarity
colSums(is.na(engage3))
```

```{r, time-1044, echo=TRUE}
engage3 <- engage3 %>%
  mutate(time = replace(time, time == "1/01/2050", "1/01/2020"))
```

```{r, district-id-1044, echo=TRUE}
# add new column
engage3 <- mutate(engage3, district_id = 1044)
```

**d. 1052.csv (Illinois)**.
Similarly, we repeat the observing process for "1052.csv".
```{r, summary-1052, echo=TRUE}
engage4 = read_csv(file="Engagement Data/1052.csv") #rename for clarity

# examine the dataset rows and columns 
colSums(is.na(engage4))
```

```{r, time-1052, echo=TRUE}
engage4 <- engage4 %>%
  mutate(time = replace(time, time == "1/01/2033", "1/01/2020"))
```

```{r, district-id-1052, echo=TRUE}
# add new column
engage4 <- mutate(engage4, district_id = 1052)
```

###### Districts Data
We start with inspecting the dataset.
```{r, summary-districts, echo=TRUE}
districts = read_csv(file="districts_info.csv")
colSums(is.na(districts)) # check n/a

districts <- districts %>% # rename columns for clarity
  rename(pct_black.hispanic = "pct_black/hispanic") %>% 
  rename(pct_free.reduced = "pct_free/reduced")
```

```{r, column-district_id, echo=TRUE}
# unique(districts$district_id) # inspect the data
```

```{r, state, echo=TRUE}
  ## transform the data
districts <- districts %>%
  mutate(state = case_when(
    state %in% c("UTAH", "uTtah", "Utaah") ~ "Utah",
    state == "ConnectiCUT" ~ "Connecticut",
    state %in% c("NY City", "New Y0rk") ~ "New York",
    state == "Ohi0" ~ "Ohio",
    state == "District Of Columbia" ~ "Washington",
    state %in% c("whereabouts", "don\x92t know", "NaN") ~ NA,
    TRUE ~ state)) %>%
  filter(!is.na(state))

unique(districts$state) # verify if adjustment is made
```


```{r, locale, echo=TRUE}
  ## transform the data
districts <- districts %>%
  mutate(locale = case_when(
    locale %in% c("Sub", "C1ty") ~ "Suburb",
    locale == "Cit" ~ "City",
    TRUE ~ locale))

unique(districts$locale) # verify if adjustment is made
```

```{r, pct-black-hispanic, echo=TRUE}
  ## transform the data
districts <- districts %>%
  mutate(pct_black.hispanic = case_when(
    pct_black.hispanic == "NA" ~ NA,
    pct_black.hispanic == "[0, 0.2[" ~ "0-20%",
    pct_black.hispanic == "[0.2, 0.4[" ~ "20-40%",
    pct_black.hispanic == "[0.4, 0.6[" ~ "40-60%",
    pct_black.hispanic == "[0.6, 0.8[" ~ "60-80%",
    pct_black.hispanic == "[0.8, 1[" ~ "80-100%",
    TRUE ~ pct_black.hispanic)) %>%
  filter(!is.na(pct_black.hispanic))

unique(districts$pct_black.hispanic) # verify if adjustment is made
```

```{r, pct-free-reduced, echo=TRUE}
  ## transform the data
districts <- districts %>%
  mutate(pct_free.reduced = case_when(
    pct_free.reduced == "NA" ~ NA,
    pct_free.reduced == "[0, 0.2[" ~ "0-20%",
    pct_free.reduced == "[0.2, 0.4[" ~ "20-40%",
    pct_free.reduced == "[0.4, 0.6[" ~ "40-60%",
    pct_free.reduced == "[0.6, 0.8[" ~ "60-80%",
    pct_free.reduced == "[0.8, 1[" ~ "80-100%",
    TRUE ~ pct_free.reduced)) %>%
  filter(!is.na(pct_free.reduced))

unique(districts$pct_free.reduced) # verify if adjustment is made
```

```{r, county-connections-ratio, echo=TRUE}
  ## transform the data
districts <- districts %>%
  mutate(county_connections_ratio = case_when(
    county_connections_ratio == "[0.18, 1[" ~ "<1",
    county_connections_ratio == "[1, 2[" ~ ">1",
    county_connections_ratio == "NA" ~ "NaN",
    TRUE ~ county_connections_ratio))

unique(districts$county_connections_ratio) # verify if adjustment is made
```

```{r, pp-total-raw, echo=TRUE}
  ## transform the data
districts <- districts %>%
  mutate(
    pp_total_raw = case_when(
      pp_total_raw == "NA" ~ "NaN",
      pp_total_raw == "[4000, 6000[" ~ "4-6",
      pp_total_raw == "[6000, 8000[" ~ "6-8",
      pp_total_raw == "[8000, 10000[" ~ "8-10",
      pp_total_raw == "[10000, 12000[" ~ "10-12",
      pp_total_raw == "[12000, 14000[" ~ "12-14",
      pp_total_raw == "[14000, 16000[" ~ "14-16",
      pp_total_raw == "[16000, 18000[" ~ "16-18",
      pp_total_raw == "[18000, 20000[" ~ "18-20",
      pp_total_raw == "[20000, 22000[" ~ "20-22",
      pp_total_raw == "[22000, 24000[" ~ "22-24",
      pp_total_raw == "[32000, 34000[" ~ "32-34",
      TRUE ~ pp_total_raw))

unique(districts$pp_total_raw) # verify if adjustment is made
```

##### Products Data
```{r, summary-products, echo=TRUE}
products = read_csv(file="products_info.csv")
colSums(is.na(products))

products <- products %>% # rename columns
  rename(product_name = "Product Name") %>%
  rename(provider.company_name = "Provider/Company Name") %>%
  rename(sector = "Sector(s)") %>%
  rename(lp_id = "LP ID") %>%
  rename(primary_essential_function = "Primary Essential Function")
```

```{r, product-name, echo=TRUE}
products <- products %>%
  filter(!is.na(product_name)) #dropping n/a
```
  
```{r, sector, echo=TRUE}
products <- products %>%
  mutate(sector = case_when(
    sector %in% c("PreK-122", "PreK-112", "PPreK-12", "pre kindergarten to yr 12", 
                  "pre kindergarten to year 12") ~ "PreK-12",
    sector == "PreK-12; Higher; Corporate" ~ "PreK-12; Higher Ed; Corporate",
    sector == "not sure" ~ NA,
    TRUE ~ sector))

unique(products$sector)
```
  
```{r, primary-essential-function, echo=TRUE}
# Extract function part
products$function_type <- sub("\\s*-.*", "", products$primary_essential_function)

  ## transform the data
products <- products %>%
  mutate(function_type = replace(function_type, function_type == "CL", "LC"))
unique(products$function_type) #verify if adjustment is made


# Extract sub-function part
products$function_subtype <- sub(".*-\\s*", "", products$primary_essential_function)
```

### 1.2 Wrangling

```{r, wrangling, echo=TRUE}
engagement <- bind_rows(engage1, engage2, engage3, engage4)

engagement <- engagement %>%
  left_join(districts, by = "district_id") # merge engagement with districts 

engagement <- engagement %>%
  left_join(products, by = "lp_id") # merge engagement with products
```

 
\newpage


# 2. Data Visualisation
```{r, set-up-2, echo = FALSE}
# import necessary libraries
library(zoo)
library(viridis) # provides a set of color palettes for the visualizations
```

### Section 1: Engagement During 2020
To explore the trend in 2020, a bar-line graph has been utilized. This combined graph format provides a comprehensive visualization by integrating two types of data representations: bars and lines. The bars represent the daily means of engagement metrics, offering a clear overview of the average engagement levels across different months. Meanwhile, the line graph outlines trends over time, allowing for a deeper analysis of the engagement patterns, especially regarding the pandemic outbreak or the school break.

```{r, plot-1, echo=FALSE}
# Extract "month" values 
engagement$time <- as.Date(engagement$time, format = "%d/%m/%Y") 
engagement$month <- format(engagement$time, "%m")
engagement <- engagement[!is.na(engagement$month), ] # drop n/a values in the month column

# Calculate the mean pct_access
monthly_mean <- engagement %>%
  group_by(month) %>%
  summarise(mean_pct_access = mean(pct_access, na.rm = TRUE))

# Plot
ggplot(monthly_mean, aes(x = month, y = mean_pct_access)) +
  geom_bar(stat = "identity", fill = "lightblue", alpha = 0.5) +  # Bar plot
  geom_line(color = "skyblue4", group = 1) +  # Line plot
  geom_point(aes(y = mean_pct_access), color = "gold") + # Dots on the line
  labs(title = "Mean Daily Accessed Products in 2020",
       x = "Month",
       y = "Mean Daily Accessed Products") +
  scale_x_discrete(labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", 
                              "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


### Section 2: Learning Platforms
**2.1 Top Tools**.
Regarding categorical values, a bar graph is considered the most effective as it allows for better comparison between different categories.

```{r, plot-2-1, echo=FALSE}
# Convert to data frame for plotting
product_counts <- table(engagement$product_name)
product_counts_df <- data.frame(product_name = names(product_counts),
                                count = as.numeric(product_counts))

# Sort the data by count in descending order and select top 10
top_10 <- product_counts_df[order(-product_counts_df$count), ][1:10, ]

# Plot "Top 10 Most Popular Tools in 2020"
ggplot(top_10, aes(x = reorder(product_name, -count), y = count)) +
  geom_bar(stat = "identity", fill = "skyblue3") +
  labs(title = "Top 10 Most Popular Tools in 2020",
       x = "Product Name",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**2.2 Top 10 Providers** 
```{r, plot-2-2, echo=FALSE}
# Convert to data frame for plotting
provider_counts <- table(engagement$provider.company_name) 
# count the occurrences of each provider

provider_counts_df <- data.frame(provider_company_name = names(provider_counts),
                                 count = as.numeric(provider_counts)) # converting

# Sort the data by count in descending order and select top 10
top10_provider <- provider_counts_df[order(-provider_counts_df$count), ][1:10, ]

# Plot "Top 10 Providers in the US in 2020"
ggplot(top10_provider, aes(x = reorder(provider_company_name, -count), y = count)) +
  geom_bar(stat = "identity", fill = "darkgoldenrod2") +
  labs(title = "Top 10 Providers in the US in 2020",
       x = "Provider Company Name",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**2.3 Top Sectors**
```{r, plot-2-3, echo=FALSE}
sector_df <- engagement %>%
  filter(!is.na(sector)) # clean the data
  
ggplot(sector_df, aes(x = sector)) +
  geom_bar(stat = "count", fill = "lightskyblue3") +  # count occurrences of each sector
  coord_flip() +
  labs(title = "Number of Products By Sector(s)",
       x = "Sector(s)",
       y = "Number of Products") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Section 3: Product Function and Engagement
In the context of examining the distribution of three primary function types (Learning & Curriculum, Classroom Management, and School & District Operations), a stacked bar graph is likely to be the most suitable. Although a pie chart was initially considered, it has been found (Hunt, 2019) that pie charts may not be optimal due to several reasons: (1) it is challenging for the audience to estimate accurately; (2) they complicate the matching of labels to slices; and (3) they may not effectively visualize small percentages. 

Meanwhile, stacked bar graphs offer a more intuitive way to communicate data, as they allow for a direct comparison of the quantities of each function type and sub-type within a single bar. Additionally, one application of a stacked bar graph is enabling readers to examine the rankings of items according to multiple attributes (Gratzl et al., 2013). Therefore, a stacked bar graph appears as the preferable choice.

**3.1 Primary Functions**.
```{r, plot-3-1, echo=FALSE}
# Replace to prepare for visualization
engagement <- engagement %>%
  mutate(function_type = replace(function_type, function_type == "CL", "LC"))

# Prepare the dataframe 
function_type_df <- engagement %>%
  filter(!is.na(function_type)) %>% # clean the data
  filter(!is.na(engagement_index)) %>% # clean the data
  group_by(function_type) %>% # group the data according to its function type
  summarise(Sum_Engagement = sum(engagement_index)) %>% # calculate the sum of engagement index for each function type
  mutate(Percentage = Sum_Engagement / sum(Sum_Engagement) * 100) %>% # covert the sum to percentage
  arrange(desc(Percentage)) %>% # sort the data in descending order
  mutate(function_type = factor(function_type, levels = rev(unique(function_type))))


# Plot "Primary Function Type & Engagement Index"
custom_palette <- c("#B5D9E5", "#F6DBC0", "#F0BB71", "#C57700")
ggplot(function_type_df, aes(x = "", y = Percentage, fill = function_type)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage), "%")), 
            position = position_stack(vjust = 0.5), 
            size = 3) +  
  coord_flip() +  
  scale_fill_manual(values = custom_palette) +
  labs(x = NULL, y = "Percentage", fill = "Function Type", title = "Primary Function Type & Engagement") +
  theme_minimal() +
  theme(legend.position = "right",
        legend.text = element_text(size = 8))
```


**3.2 Sub-types** 
```{r, plot-3-2, echo=FALSE}
# Prepare the dataframe
function_subtype_df <- engagement %>%
  filter(!is.na(function_subtype)) %>% # clean the data
  filter(!is.na(engagement_index)) %>% # clean the data
  group_by(function_subtype) %>% # group the data by its sub-types
  summarise(Sum_Engagement = sum(engagement_index)) %>% # calculate the sum of engagement index for each sub-types
  mutate(Percentage = Sum_Engagement / sum(Sum_Engagement) * 100) %>% # covert the sum to percentage
  arrange(desc(Sum_Engagement)) %>% # sort the data in descending order
  top_n(5) # select top 5 function subtypes based on engagement index sum


# Plot "Top 5 Sub-Type Functions"
custom_palette <- c("#D5E4D7", "#B5D9E5", "#F6DBC0", "#F0BB71", "#C57700")
ggplot(function_subtype_df, aes(x = "", y = Percentage, fill = function_subtype)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage), "%")), 
            position = position_stack(vjust = 0.5), 
            size = 3) +  
  coord_flip() +  
  scale_fill_manual(values = custom_palette) + 
  labs(x = NULL, y = "Percentage", fill = "Function Subtype", title = "Top 5 Sub-Types Based on Engagement Index") +
  theme_minimal() +
  theme(legend.position = "right",
        legend.text = element_text(size = 8))

```

### Section 4: Geographic Context
**4.1**.
For the purpose of comparing the number of districts, bar graphs makes it straightforward to interpret and compare the data at a glance.

```{r, plot-4-1, echo=FALSE}
district_count <- districts %>%
  filter(!is.na(state)) # clean the specified "data"

# Plot "Number of Districts per State"
district_count %>%
  count(state) %>%
  ggplot(aes(x = reorder(state, n), y = n, fill = state)) +
  geom_bar(stat = "identity", fill = "lightskyblue3") +
  geom_text(aes(label = n), vjust = -0.5, color = "gray31", size = 3) +
  scale_fill_viridis(discrete = TRUE) +
  labs(x = "State", y = "Number of Districts", title = "Number of Districts per State") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

**4.2**.
Similarly,  as we need to clearly showcase the percentage of city/suburb/rural area, a stacked bar graph should be the best fit as it allows for a visual breakdown of each category's contribution to the whole.

```{r, plot-4-2, echo=FALSE}
locale_df <- districts %>%
  filter(!is.na(locale))  # clean the specified "data"

# Convert to data frame for plotting
locale_counts <- table(locale_df$locale)
locale_counts_df <- data.frame(Locale = names(locale_counts),
                               Count = as.numeric(locale_counts))

# Calculate percentage of each locale and sort the dataframe
locale_counts_df$Percentage <- locale_counts_df$Count / sum(locale_counts_df$Count) * 100 
locale_counts_df <- locale_counts_df[order(-locale_counts_df$Percentage), ] # sort the dataframe in descending order

# Plot "Location (City/Suburb/Rural)"
custom_palette <- c("#B5D9E5", "#F6DBC0", "#F0BB71", "#C57700")
ggplot(locale_counts_df, aes(x = "", y = Percentage, fill = Locale)) +
  geom_bar(stat = "identity", width = 1) +
  coord_flip() +  # flip to create a horizontal bar chart
  scale_fill_manual(values = custom_palette) +  # specify color palette
  geom_text(aes(label = paste0(round(Percentage), "%")), 
            position = position_stack(vjust = 0.5), 
            size = 3) + 
  labs(x = NULL, y = "Percentage", fill = "Locale", title = "Location (City/Suburb/Rural)",) +
  theme_minimal() +
  theme(legend.position = "right",
        legend.text = element_text(size = 8))
```


**4.3** 
```{r, plot-4-3, echo=FALSE}
# Filter engagement data for selected states
engagement_filtered <- engagement %>%
  filter(state %in% c("Connecticut", "Missouri", "Illinois"))

# Calculate the mean of engagement index grouped by products
top_products <- engagement_filtered %>%
  group_by(product_name) %>%
  summarise(mean_engagement = mean(engagement_index)) %>%
  top_n(10, mean_engagement)

# Filter engagement data for only top 10 products
engagement_top_products <- engagement_filtered %>%
  filter(product_name %in% top_products$product_name)

# Plot
custom_palette <- c("#F6DBC0", "#F0BB71", "#C57700")
combined_plots <- engagement_top_products %>%
  ggplot(aes(x = engagement_index, y = fct_reorder(product_name, engagement_index), fill = state)) +
  geom_bar(stat = "identity") +
  facet_wrap(~state, ncol = 3) +
  scale_fill_manual(values = custom_palette) +
  theme_minimal() +
  labs(title = "Mean Engagement Index for Top 10 Products by State", x = "Mean Engagement Index", y = "Product Name") +
  theme(axis.text.y = element_text(hjust = 0))

print(combined_plots)
```


### Section 5: Demographic Context
**5.1 ** 
Similarly, bar graphs are still efficient, especially since districts can be categorized by percentage range. Placing these graphs side by side helps viewers quickly identify patterns or discrepancies in the number of districts per range. 

```{r, plot-5-1-1, echo=FALSE}
# Count the number of districts with the same value in pct_black.hispanic and pct_free.reduced
district_counts <- districts %>%
  group_by(pct_black.hispanic) %>%
  summarize(count = n())

district_counts_free_reduced <- districts %>%
  group_by(pct_free.reduced) %>%
  summarize(count = n())

unique(districts$pct_black.hispanic) # verify
unique(districts$pct_free.reduced) # verify


# Create a combined data frame for pct_black.hispanic and pct_free.reduced
combined_counts <- data.frame(
  variable = rep(c("pct_black.hispanic", "pct_free.reduced"), each = nrow(district_counts)),
  value = rep(c(district_counts$pct_black.hispanic, district_counts_free_reduced$pct_free.reduced), 2),
  count = c(district_counts$count, district_counts_free_reduced$count)
)

# Plot ""School Districts by Demographic Context")
ggplot(combined_counts, aes(x = value, y = count, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = count), vjust = -0.5, color = "grey31", size = 3, position = position_dodge(width = 0.9)) + 
  labs(x = "Range Groups", y = "Count", fill = "Variable", 
       title = "School Districts by Black/Hispanic Students and 
       Students Eligible for Free/Reduced-Price Lunch") +
  scale_fill_manual(values = c("pct_black.hispanic" = "darkgoldenrod1", "pct_free.reduced" = "lightblue2")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1) +
  facet_wrap(~variable, scales = "free", nrow = 1))
```

We present a table for the audience to conveniently locate where the engagement datasets are in a bigger picture.

```{r, plot-5-1-2, echo=TRUE}
filtered_districts <- districts %>% #filter data
  filter(district_id %in% c(1000, 1044, 1052))
print(filtered_districts)
```

**5.2 **.
Similarly, a bar graph can once again be utilized to effectively present this type of data. Given that the percentage of students is retrieved from 'pct_access', we calculate the mean for 2020 and compare them between the 3 districts:

- 1000: Connecticut (Suburb)
- 1044: Missouri (Suburb)
- 1052: Illinois (Suburb)

Due to limitations, we may consider each district as representative of its respective state. 

```{r, plot-5-2-1, echo=FALSE}
# Prepare the mean of access percentage in district 1000, 1044, and 1052
chosen_district <- engagement %>%
  filter(district_id %in% c(1000, 1044, 1052)) # filter only valid district

mean_access <- chosen_district %>% # calculate the mean and convert to percentage value
  group_by(district_id) %>%
  summarise(mean_pct_access = mean(pct_access, na.rm = TRUE)*100) 

# Plot "Percentage of Students Accessing Product Webpage by District"
custom_palette <- c("#B5D9E5", "#F6DBC0", "#F0BB71")
ggplot(mean_access, aes(x = factor(district_id), y = mean_pct_access, fill = factor(district_id))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(mean_pct_access), "%")), vjust = -0.5, size = 3) +  # Add percentage labels on top of bars
  labs(x = "District ID", y = "Percentage of Students (%)",
       title = "Students Accessing Webpage by District") +
  scale_fill_manual(values = custom_palette) +  # Use custom color palette
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "top",  # Move legend to the top
        legend.title = element_blank())  # Remove legend title
```

**5.3**.
Similarly, as we have 3 categories for connection speed (">1", "<1", and "NA"), a bar graph will be an effective communication way to the audiences.

```{r, plot-5-3-1, echo=FALSE}
county_connection <- districts %>%
  filter(!is.na(county_connections_ratio)) # drop n/a

# Count the number of school districts for each connection speed category
connection_counts <- districts %>%
  group_by(connection_category = case_when(
    county_connections_ratio == "NaN" ~ "NaN",
    county_connections_ratio == ">1" ~ ">1",
    county_connections_ratio == "<1" ~ "<1"
  )) %>%
  summarise(Count = n())

# Plot "School Districts by Connection Speed")
ggplot(connection_counts, aes(x = connection_category, y = Count, fill = connection_category)) +
  geom_bar(stat = "identity") +
  labs(x = "Connection Speed Category", y = "Number of School Districts", fill = "Connection Speed Category",
       title = "School Districts by Connection Speed") +
  scale_fill_manual(values = c("darkgoldenrod1", "lightblue2", "grey")) + 
  theme_minimal()
```

To take a deeper look at the outlier with a connection speed greater than 1, we retrieve the data into the following table.

```{r, plot-5-3-2, echo=TRUE}
# Districts with county_connections_ratio = ">1"
districts_greater_than_1 <- districts %>%
  filter(county_connections_ratio == ">1")

districts_greater_than_1
```


\newpage


# 3. Findings & Recommendation
### 1. Trends of Student Engagement during 2020
The pandemic outbreak prompted a significant shift in educational practices, with educators globally transitioning to online platforms. Analysis of mean daily accessed products in 2020 reveals lower means in June, July, and August coinciding with typical school holidays, suggesting reduced academic engagement during vacation periods. This trend underscores the influence of external factors like school breaks on student online activity and highlights the adaptability of educational delivery methods during challenging times.

### 2. Learning Platforms
Among the top 10 popular tools, Google's suite including Docs, Drive, Classroom, and Sites holds a prominent position, alongside child-focused resources like ABCya! and Scholastic Inc., and essential platforms like Wikipedia and Dictionary.com. Notably, Google LLC leads the list as the top provider of the year, surpassing competitors in product accesses. Additionally, the analysis highlights PreK-12 as the dominant sector, emphasizing the prevalence of digital educational activities for younger learners. This finding underscores the prevalence of educational activities catering to younger learners within the digital landscape.

### 3. Primary Types & Sub-Types and Engagement Index
Among the three function types (Learning & Curriculum, Classroom Management, and School & District Operations), Learning & Curriculum dominates with 67% of the engagement index, exceeding the total of the other two. Content Creation & Curation, Learning Management Systems, and Digital Learning Platforms stand out as the most engaging sub-types, emphasizing the significance of educational content and platforms in driving student engagement, guiding educators and developers.

### 4. Geographic Context
From the graph 4.1, 4.2 and 4.3:
- Connecticut, Utah, and Illinois have the highest numbers of districts. District 1000 is located in Connecticut, while District 1052 is from Illinois. Meanwhile, District 1044 is situated in Missouri, a state with a district count ranking in the middle of the list.
- While most of the districts are located in suburb areas, all districts with given engagement dataset are also suburban. 
- There is a similarity regarding popular products for three districts of three different states. Thus, the geographical impact is not portraited clearly given this dataset.
Overall, we observe a limitation as the investigation restricted around suburban areas and s small sample size, which hinder a comprehensive analysis.

### 5. Demographic Context
Illinois shows the highest average percentage of daily webpage access (92%), while District 1000 (Connecticut) and District 1044 (Missouri) exhibit notably lower rates at 51% and 32% respectively, despite all being suburban districts. While this trend aligns with Waller et al.(2020)'s argument that the pandemic exacerbated societal inequalities, further investigation is warranted. District 1052, with the highest access rate, has moderate percentages of Black/Hispanic students and students on free/reduced lunch, whereas District 1000's high percentages in these categories offer partial support to the argument. The dataset's bias towards districts with lower percentages of Black/Hispanic students and students on free/reduced lunch should be considered, alongside the limited focus on demographic factors like household income and type.

### 6. Recommendations
- 6.1 For educators. They are suggested to take advantage of a variety of educational resources beyond traditional textbooks and be mindful of the varying access rates to online resources among districts, which may reflect socioeconomic disparities. Additionally, although the dataset may lack clear geographical impacts, leverage insights from demographic data to tailor educational approaches. 
- 6.2 For digital providers. They are recommended to prioritize accessibility features (screen reader compatibility, language support, and user-friendly interfaces to enhance accessibility for all learners) in digital learning platforms to ensure inclusivity for students with diverse needs. Secondly, they need to encourage collaboration between educators and platform developers to create innovative educational tools. Lastly, they are encouraged to take proactive approach to mitigate biases by collecting more comprehensive demographic data. 

\newpage
**References**

Gratzl, S., Lex, A., Gehlenborg, N., Pfister, H., & Streit, M. (2013). LineUp: Visual analysis of multiattribute rankings. IEEE Transactions on Visualization and Computer Graphics, 19(12), 2277–2286. https://doi.org/10.1109/TVCG.2013.173

Hunt, C. (2019, November 25). Why you shouldn’t use pie charts. Statistical Consulting Centre. https://scc.ms.unimelb.edu.au/resources/data-visualisation-and-exploration/no_pie-charts

Stanistreet, P., Elfert, M., & Atchoarena, D. (2020). Education in the age of COVID-19: Understanding the consequences. International Review of Education, 66(5), 627–633. https://doi.org/10.1007/s11159-020-09880-9

Waller, R., Hodge, S., Holford, J., Milana, M., & Webb, S. (2020). Lifelong education, social inequality and the COVID-19 health pandemic. International Journal of Lifelong Education, 39(3), 243–246. https://doi.org/10.1080/02601370.2020.1790267